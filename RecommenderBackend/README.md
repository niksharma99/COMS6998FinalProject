
# ğŸ¬ LLM-Augmented Conversational Movie Recommender

*FAISS retrieval + fine-tuned GPT reranker + conversational taste modeling*

This backend powers an interactive, conversational movie recommender that combines:

* Dense movie embeddings (MovieLens-based)
* Real-time user taste embeddings from text input
* Vector search using FAISS
* A fine-tuned **GPT-4o-mini** reranker for personalization
* Natural-language explanations generated by GPT
* Conversational memory using a fused, evolving taste vector

Users interact through a CLI, a chat-like interface inside the UI system, or programmatically through `recommender.py`.

---

# ğŸ—‚ Project Structure (Conceptual Overview)

```
TasteEmbeddingGenerator/        # Offline movie + user embedding generation
RecommenderBackend/
â”‚
â”œâ”€â”€ recommender.py              # Core conversational recommender pipeline
â”œâ”€â”€ embedding_loader.py         # Loads embedding artifacts into memory
â”œâ”€â”€ vector_index.py             # FAISS inner-product search index
â”œâ”€â”€ gpt_reranker.py             # Optional: GPT-based reranking module
â”œâ”€â”€ llm.py                      # Lightweight wrapper for OpenAI API calls
â”œâ”€â”€ user_store.py               # Persistent taste-vector memory
â”‚
â”œâ”€â”€ letterboxd_collect_dataset.py     # Scrape/prep Letterboxd-style data
â”œâ”€â”€ letterboxd_to_finetune.py         # Convert scraped data â†’ GPT fine-tuning JSONL
â”œâ”€â”€ finetune_movie_pref.py            # Launch GPT-4o-mini fine-tune
â”‚
â”œâ”€â”€ eval_embedding_alignment.py       # Quantitative retrieval evaluation
â”œâ”€â”€ eval_qualitative_gpt.py           # GPT-as-a-judge qualitative eval
â”œâ”€â”€ qualitative_eval.py               # Human-readable qualitative samples
â”‚
â””â”€â”€ README.md                  # (this file)
```

---

# ğŸš€ System Overview

The recommender is built as a **two-stage retrieval + re-ranking architecture**, augmented with conversational memory and LLM-generated explanations.

## **1. Taste Embedding from User Input**

Users provide:

* free-text preferences (e.g., *â€œI like slow atmospheric sci-fiâ€*),
* lists of favorite movies,
* or mixed conversational feedback over time.

We embed this text using the `BAAI/bge-base-en-v1.5` SentenceTransformer model. Each embedding is normalized so cosine similarity matches inner product search in FAISS.

---

## **2. Conversational Taste Memory (EMA Fusion)**

Each user has a persistent taste vector updated using Exponential Moving Average:

[
\mathbf{u}*{t} = \alpha \mathbf{u}*{t-1} + (1 - \alpha),\mathbf{e}_{t}
]

Where:

* (\mathbf{e}_t) is the newest embedding from the userâ€™s message
* (\alpha = 0.8) gives strong long-term memory with flexible short-term adjustments

This enables:

* multi-turn preference refinement
* user profiles that evolve over the conversation
* personalization without storing raw conversation text in the model

State is persisted via `user_store.py`.

---

## **3. Candidate Retrieval Using FAISS (Top-20)**

Movie embeddings (precomputed offline via `TasteEmbeddingGenerator`) are loaded and indexed using:

* **FAISS IndexFlatIP** (inner-product search)
* O(1 ms) retrieval of the Top-20 most similar movies to the user taste vector

This stage provides **broad semantic recall**.

---

## **4. GPT-Based Re-Ranking (Top-5)**

To refine the Top-20 retrieved items, we apply an LLM-based reranker using a **custom fine-tuned GPT-4o-mini model**, trained to predict how much a user would rate a movie (1â€“5).

The reranker considers:

* fused conversational taste history
* movie metadata (genres, plot, year, themes)
* fine-tuned preference-prediction model outputs

Two reranking implementations are available:

### **(a) GPT-Generated Natural-Language Ranking** (default)

`recommender.py` produces:

* a prompt containing user history + candidate movies
* GPT selects the **Top-5** and explains why

### **(b) Numerical Reranker (predict_like_score + combined_score)**

`gpt_reranker.py` computes:

[
\text{final score} = \alpha \cdot \text{cosine similarity} + (1 - \alpha)\cdot \hat{y}_{GPT}
]

Then selects the highest-scoring Top-5.

Either mode can be enabled depending on evaluation needs.

---

## **5. GPT Explanations**

Every final recommendation includes:

* personalized justification
* references to user preference history
* brief film descriptions

This gives the system **interpretability** without requiring model changes.

---

## **6. Logging + Offline Evaluation**

`recommender.py` logs every interaction:

* taste vectors
* candidate indices + scores
* final Top-5 selections
* conversation step number (msg_index)

Evaluation scripts use these logs for quantitative and qualitative analysis.

---

# ğŸ“š Training Details (GPT-4o-mini Fine-Tuning)

We fine-tuned **GPT-4o-mini** to act as a personalized movie preference predictor.

### **Dataset Preparation**

Scripts used:

* `letterboxd_collect_dataset.py`
  â†’ collects user rating & review data (Letterboxd-style)

* `letterboxd_to_finetune.py`
  â†’ converts each example into training pairs:

```
User Profile + Candidate Movie â†’ Expected Rating (1â€“5)
```

Fine-tuning examples follow the OpenAI chat JSONL format:

```json
{"messages": [
  {"role": "user", "content": "User profile...\nMovie metadata..."},
  {"role": "assistant", "content": "4"}
]}
```

### **Training Objective**

The model learns the function:

[
f_{\theta}(\text{user}, \text{movie}) \rightarrow {1,2,3,4,5}
]

This allows GPT to:

* predict rating tendencies
* detect subtle preference patterns
* improve relevance and personalization in reranking

### **Fine-Tuning Execution**

`finetune_movie_pref.py` launches:

```bash
openai api fine_tuning.jobs.create \
  -m gpt-4o-mini \
  -t finetune_dataset.jsonl
```

The resulting model ID (e.g. `ft:gpt-4o-mini:movie-pref-ranker`) is stored for inference.

### **Deployment**

The recommender uses the fine-tuned model during reranking via:

```python
predict_like_score(...)
combined_score(...)
```

or via the natural language ranking step inside `recommender.py`.

---

# ğŸ“Š Evaluation Overview (High-Level)

Evaluation scripts include:

## **Quantitative (Embedding Alignment)**

`eval_embedding_alignment.py`

* Computes mean cosine similarity between taste vectors and Top-5 recommendations
* Compares:

  * embedding-only retrieval
  * GPT-reranked retrieval
* Measures stability, variance, and alignment

## **Qualitative (GPT-as-a-Judge)**

`eval_qualitative_gpt.py`

* Provides GPT with conversations + explanations
* Scores:

  * relevance
  * personalization
  * diversity
  * clarity
  * overall satisfaction
* Enables controlled comparison of different system variants

## **Human-Readable Summaries**

`qualitative_eval.py` outputs example conversations + explanations for inspection.

(No evaluation numbers are included in this README, only the methodology.)

---

# ğŸ› Running the CLI Demo

```bash
cd RecommenderBackend
python test_cli.py
```

The system will:

* embed your preference text
* fuse it into your taste history
* retrieve Top-20 movies
* rerank to Top-5 using GPT
* produce explanations

---


---


